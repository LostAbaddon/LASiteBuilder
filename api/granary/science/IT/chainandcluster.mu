TITLE：链与云的对比
AUTHOR：LostAbaddon
DATE：2021/09/24 09:30
KEYWORD：区块链

本文主要针对公链、联盟链与云服务器集群在理论响应速度上进行了模型测算，并做了一定的数值对比。

当然，由于未做实际测试，所以一切都只是理论值。

[TOC]

#	理论模型

我们先来构造恰当的理论模型，并以此为基础计算相应的响应理论值。

##	云服务器集群

在传统云服务器集群中，一般有如下这些组件：

1.	负载均衡
	负载均衡还会分两层，一层是对外网的，一层是对内网的
2.	业务处理子集群
	每个子集群都只处理特定的一类业务，随后提交给上层做整合与进一步处理
	其中最常用的就是数据库（包括MySQL、Redis等）的子集群
	-	主从架构
		主节点负责写，从节点负责读
		主节点可以是人为设置的，也可以是动态选举出来的（比如RAFT算法等）

对于复杂系统来说，上述结构往往只是一个工作单元，若干个这样的工作单元可以构成上一层的网络，这样的网络可以有多层。

我们下面只考虑一层的情况，此时业务响应时间为：

$$
T_{Resp} \approx T_{Outside} + T_{Balance} + T_{Insider} + \left( \left\lfloor \frac{N_{Pending}}{N_{Node} N_{Parellel}} \right\rfloor + 1 \right) T_{Tx}
$$

这里$T_{Outside}$是外网通讯时长，不是集群可以控制的；$T_{Balance}$是负载均衡处理信息派发所用的时长，一般可以忽略；$T_{Insider}$是内网通讯所用的时长，当集群在同一个机房或网络中时，这个时长也可以忽略，但如果是跨网络的，这个时长则可能会较显著；$T_{Tx}$是节点处理业务的平均时长，一般是一个集群中最耗时的部分。$N_{Pending}$所有正处于等待状态的业务量，$N_{Node}$是业务节点的数量，$N_{Parellel}$是节点并发能力。

这里我们还考虑了单一节点对多任务的处理时的并发情况，显然当节点足够多时，负载均衡会将业务平均派发给各个节点（均衡策略不同，这个部分也会不同，我们这里做了最简单的处理：每个节点的处理速度都差不多，每个业务请求的复杂度也差不多），所以新业务请求会被安排给目前等待处理业务最少的节点，并会等到前面的业务请求完成后再进行处理。

下面，假定现在单位时间内涌入的业务量为$N_{Tx}$，其之间业务队列为空，那么当$N_{Tx} > N_{Node} N_{Parellel}$时，第一波请求不用等待就能被处理，而之后的请求会开始进入等待模式，从而最长的等待时长差不多为：

$$
T_{MaxResp} \approx T_{Outside} + T_{Balance} + T_{Insider} + \left\lceil \frac{N_{Tx}}{N_{Node} N_{Parellel}} \right\rceil T_{Tx}
$$

所以系统并发能力就是：

$$
P \approx \frac{N_{Tx}}{T_{MaxResp}}
$$

实际情况下，因为等待队列未必为空，所以并发数不会超过这个值。

在理想状态下，并发量可以进一步被简化为：

$$
P \approx \frac{N_{Tx}}{\left\lceil \frac{N_{Tx}}{N_{Node} N_{Parellel}} \right\rceil T_{Tx}} \approx \frac{N_{Node} N_{Parellel}}{T_{Tx}}
$$

所以，业务处理时长越短、节点以及节点上的并发处理能力越强，系统的并发量就可以越大。

当然，实际情况中往往需要考虑数据库读写，尤其数据库基本是一主多从的架构，所以我们有必要对此进行进一步拆分（忽略所有可以忽略的时间）：

$$
T_{MaxResp} \approx \left\lceil \frac{N_{Tx}}{N_{TN} N_{TP}} \right\rceil \left( T_{Tx} + \left\lceil \frac{N_{TN} N_{TP}}{N_{DP}} \right\rceil T_{Db} \right)
$$

这里$N_{Tx}$依然是业务请求总数，$N_{TN}$是业务逻辑处理节点总数，$N_{TP}$是业务逻辑处理服务器的并发能力，$N_{DP}$是数据库主节点的并发能力（不单单是硬件能力，还包括数据库允许的并发情况，视分库情况以及数据库类型而定），$T_{Tx}$是业务逻辑处理时长，$T_{Db}$是数据库处理时长。

同样的，系统整体并发量现在为：

$$
P \approx \frac{ N_{TN} N_{TP} N_{DP} }{ N_{DP} T_{Tx} + N_{TN} N_{TP} T_{Db} }
$$

可见，现在业务逻辑处理时长短到一定程度后，就不再是整个系统的最大瓶颈，数据库写操作是最大的瓶颈（我们上面基本将读操作的时间给忽略了）。此时，良好的分库分表比提升业务逻辑处理速度更重要。而且这里也可以看出，当数据库操作不多的时候，通过增加业务逻辑处理节点可以有效缩短系统整体的响应时长增大系统并发量，但当结果过多的时候，所有业务逻辑处理节点都向数据库发出写操作，会使得这块的用时显著增大。因此集群达到一定规模的时候，增加业务节点带来的边际收益是递减的。

##	公链

公链中的节点，一般都分为两类：

1.	工作节点（矿工）
2.	其它

其它节点包括很多，比如比特币网络中只保存区块头的验证节点或者说轻节点，以及连验证都不参与只发送交易请求的用户节点。

公链中一般不会将工作节点区分为只负责智能合约的事务节点与只生成区块的区块节点（联盟链中会做此区分），所以在公链中我们不对这两类节点做区分。

在公链上，无论共识采用的是PoW、PoS、DPoS还是DPoS+PBFT，一个业务的处理流程基本为如下情况：

1.	请求发送给超过一半的工作节点（PoW共识可容纳的离线节点数不得超过50%，这还不考虑收到请求的节点里存在拜占庭节点的情况）
2.	工作节点完成事务处理，将结果写入预备区块
3.	工作节点实现共识算法
4.	将预备区块广播
5.	收敛机制确认新区块的有效性
6.	业务请求节点从区块链中获得事务被正确执行的结果

和集群相比，在第一步已经出现了显著的差异：集群中外部请求发给网关/负载均衡，又后者根据集群中每个业务节点的工作情况来找出唯一的响应节点，将请求转发过去，所以两次都是信息的直传。但在公链中，请求被发送到工作节点后，工作节点本身没有负载均衡的功能，它会将信息通过一定的广播协议转发给其它的工作节点，比如比特币采用的是Gossip协议（改良的Flooding协议）而以太坊采用Kademlia协议（一种在BT网络与IPFS上最常见的DHT协议），因此通讯不是两次直传，而差不多是$\lambda \ln(N_{W}) / \ln(n)$，这里$N_{W}$是工作节点数量而$n$是邻点数，$\lambda$是不同协议给出的传播因子，Kademlia协议的$\lambda$理论上比Gossip协议的要小。

因此，在这一步，集群的用时为$T_{Outside} + T_{Balance} + T_{Insider}$，而在公链上就是：

$$
T_{Outside} + \lambda \frac{ \ln(N_{W}) }{ \ln(n) } T_{Insider}
$$

值得注意的是，公链的工作节点之间并没有特别组网，所以这里的内网通讯时间未必是一个小值。这还不算公链中要同步的区块体积过大时带来的网络延迟影响。

>	比特币网络各节点必须在10分钟的出块间隔内同步2MB的新块，这本来没什么，但随着交易量的增多，一些扩容方案提出无限增加区块大小的设想，只使得比特币网络在10分钟内必须全网同步的数据量大增，从而对正常交易请求会带来不容忽视的网络通讯延迟。

在业务处理方面，公链的工作节点必须在完成共识（也即“挖矿”）的同时处理业务，这样前者因为占用节点计算资源，所以对后者的影响会很大。再加上公链工作节点的机器性能层次不齐，专用矿机甚至在CPU处理能力上很弱，所以这里的$T_{Tx}$一般都远大于集群中的$T_{Tx}$。

因此，到广播预备块之前，事务处理时长为：

$$
\left\lceil \left( 1 + \left\lfloor \frac{N_{Tx}}{N_{TP}} \right\rfloor \right) \frac{T_{Tx}}{T_{Con}} \right\rceil T_{Con}
$$

这里$T_{Con}$是共识所用时长。预备区块的广播所用时间和第一步中的时间形式上相同，但通讯时长本身需要考虑区块自身大小对网络带来的影响：

$$
\lambda \frac{ \ln(N_{W}) }{ \ln(n) } \left( \frac{N_{Bk} S_{Tx} + S_{Head}}{V_{Com}} \right)
$$

这里$N_{Bk}$是每个区块中打包的事务数量，由数据结构与区块体积决定。$S_{Tx}$是交易数据大小，$S_{Head}$是区块内其它数据的体积，$V_{Com}$是工作节点之间的平均通讯速度。

最后再加上确认新块的时长$(C - 1) T_{Con}$，所以最终事务所用总时长为：

$$
T_{Resp} \approx T_{Outside} + \lambda \frac{ \ln(N_{W}) }{ \ln(n) } \left( T_{Insider} + \frac{N_{Bk} S_{Tx} + S_{Head}}{V_{Com}} \right) + \frac{1}{N_{Bk}} \left\lceil \left\lfloor \frac{N_{Tx}}{N_{TP}} \right\rfloor \frac{T_{Tx}}{T_{Con}} + C \right\rceil T_{Con}
$$

由于公链的特性，这里唯一能被忽略的只有外部通讯时间以及初次内部同步请求的耗时，内部广播区块的用时因为存在传递区块而带来的延迟，所以一般不能被简单地忽略。

限制公链吞吐量的关键，在于区块本身的容量、共识产生的速度以及工作节点之间通讯用时这几点。公链吞吐量可以近似表达为：

$$
P \approx \left[ \lambda \frac{ \ln(N_{W}) }{ \ln(n) } \frac{ S_{Tx} }{ V_{Com} } N_{Bk} + \frac{1}{N_{Bk}} \left( \frac{T_{Tx}}{N_{Tp}} + \frac{C T_{Con}}{N_{Tx}} \right) \right]^{-1}
$$

也就是说，区块扩容是有一个效率极限的：

$$
N_{Bk, opt} \approx \sqrt{ \frac{ \ln(n) }{ \ln(N_{W}) } \frac{ V_{Com} }{ S_{Tx} \lambda } \left( \frac{T_{Tx}}{N_{Tp}} + \frac{C T_{Con}}{N_{Tx}} \right) }
$$

大于或小于这个极限的时候公链的事务吞吐量都会小于极限状态下的最佳值：

$$
P_{opt} \approx \frac{1}{2} \sqrt{ \lambda \frac{ \ln(N_{W}) }{ \ln(n) } \frac{ S_{Tx} }{ V_{Com} } \left( \frac{T_{Tx}}{N_{Tp}} + \frac{C T_{Con}}{N_{Tx}} \right) }^{-1}
$$

现在，我们可以总结公链的特性为：

1.	增加工作节点不会带来显著的提速，反而会增加网络通讯延迟
2.	共识花费时间的影响巨大
3.	通过提升区块体积（扩容）可以有效增加吞吐量，但存在一个极限，大约该极限后吞吐量反而会降低

##	联盟链

联盟链的情况与公链类似，但联盟链中一般会将工作节点根据职能做进一步划分：

1.	产生共识的节点
2.	记录区块链的节点
3.	处理业务的节点

比如，在Hyperledger中这三类节点分别为Orderer、Committer和Endorser。

>	从功能上说，Endorser类似处理具体业务逻辑的业务线程，Committer类似专门写数据库的主节点，而Orderer则是全局唯一的MQ。

而且，在不同节点之间采用的通讯协议也可以不同，比如Hyperledger中Endorser和Committer这两个群体之间采用的是Gossip协议，而在Orderer内部采用的是Kafka网络。

从吞吐量来说，联盟链的吞吐量与公链在数学形式上是类似的，只不过具体取值会有不同，比如事务处理速度可以和集群中的相当。

我们可以显然地发现，之前讨论的问题在Hyperledger中都会存在，比如Orderer节点可以通过为区块增容来提升整体吞吐量，而节点过多则会导致整体吞吐量反而下降，这些都是在公链吞吐量的表达中我们已经预见到的问题。


#	比较

我们下面给出集群与链的吞吐量比较：

$$
\begin{cases}
P_{Cluster} \approx \frac{ N_{TN} N_{TP} N_{DP} }{ N_{DP} T_{Tx} + N_{TN} N_{TP} T_{Db} }\\
P_{Chain} \approx \left[ \lambda \frac{ \ln(N_{W}) }{ \ln(n) } \frac{ S_{Tx} }{ V_{Com} } N_{Bk} + \frac{1}{N_{Bk}} \left( \frac{T_{Tx}}{N_{Tp}} + \frac{C T_{Con}}{N_{Tx}} \right) \right]^{-1}\\
P_{Chain, opt} \approx \frac{1}{2} \sqrt{ \lambda \frac{ \ln(N_{W}) }{ \ln(n) } \frac{ S_{Tx} }{ V_{Com} } \left( \frac{T'_{Tx}}{N_{Tp}} + \frac{C T_{Con}}{N_{Tx}} \right) }^{-1}\\
T'_{Tx} > T_{Tx}
\end{cases}
$$

下面，考虑集群有10台业务服务器，链上与10个工作节点，节点通讯时的广播邻点数为3，广播因子$\lambda$取为2（已经很大了）。集群和链的每个节点并发能力都设为4，链的通讯速度为20MBPS（4G当前的平均最高速度），单条交易记录大小为1KB，且只需要一次确认，出块速度算0.5秒（Hyperledger的理论速度）。数据写入方面，集群采用先在Redis中缓存的方式，写速度大约是每秒80000次，并发数就算1。集群和链的事务处理速度都算0.001秒（也就是1毫秒），这样当两个系统都面对1000000个请求时，两者的吞吐量分别为：

$$
\begin{cases}
P_{Cluster} \approx 26667\\
P_{Chain} \approx \left[ 0.00021 \times N_{Bk} + \frac{0.0002505}{N_{Bk}} \right]^{-1}\\
P_{Chain, opt} \approx 2182.13
\end{cases}
$$

也即，这样配置的区块链系统（基本上就是联盟链）的吞吐量比云服务器集群的吞吐量差了一个数量级。集群40秒能消化掉的请求，链需要耗时超过400秒。

造成区块链TPS低下的原因包括出块速度（主要就是共识速度）与网络同步耗时，另一方面共识机制对工作节点的业务处理速度的影响在某些情况下也需要考虑（比如让比特币网络也运行智能合约的话）。因此要提升速度就必须采取更快的共识机制，并将工作节点限制在一个快速网络内从而降低网络通讯耗时，另一方面也要在整个链的机制上进可能降低对网络通讯的依赖。此外，多层链架构理论上可以为区块链提速，其本质是将业务处理与共识验证完全剥离，并引入事后仲裁，即业务完成时链可能并没有完成验证但已将结果返回，事后可以通过仲裁机制对结果做出调整。这在可信的事务环境下是允许的，从而可以让链的速度有质上的飞跃。以太坊2.0的Layer2就是采用了这样的架构。

事实上，Hyperledger Fabric的理论处理速度为2000到3000TPS，和我们这里的估算是差不多的。在2.0中，Hyperledger Fabric重构了共识模块，提升了Orderer排序速度和Committer的验证速度以及相关I/O速度，宣称将处理速度提升为20000TPS，提升了一个数量级，那和我们这里的模拟集群的处理速度相当，但集群可以进一步通过增加业务处理节点和优化架构来提速，而Hyperledger Fabric却无法简单提速，所以这里的结论依然成立。

也因此，区块链几乎不可能被用作一个业务处理系统，而只能主要作为一个记录系统。

最后，作为比较，公链以太坊的网络传输速度会更慢而出块时间也更长（15秒），所以通过上述计算可知，其理论吞吐量极限不会超过350TPS，而实际上目前以太坊的处理速度还没超过30TPS（今年9月有人测试下来为13.58TPS，实际上在15到19之间）。在从PoW换到PoS后，理论上出块速度有了巨大的提升，Vitalik宣称以太坊2.0会达到10万TPS，但目前还没有真实数据来支撑这个宣言。而双十一时淘宝服务器集群的处理速度达到了1.4亿TPS。













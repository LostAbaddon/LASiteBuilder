标题：面对AGI的潜在危害，我们是选择停摆还是……
作者：LostAbaddon
关键词：科普 AI 人工智能
更新：2023/03/30 16:37:00

在i商周的公众号文章[《揭秘OpenAI的权利斗争·马斯克愤怒离场》](https://mp.weixin.qq.com/s?__biz=MzIyMDcwMjAwMA==&mid=2247597648&idx=1&sn=ee9bb2cacea2bbaf50988c6e079f7a3f&scene=21#wechat_redirect)中作者写道“__马斯克曾不止一次对OpenAI现在的发展方向表达过不满，最新报道指出，马斯克离开OpenAI的真实原因是夺权失败__”。

而就在此后不久，就出现了这两天响彻网络的消息：

{|}[“危险！立刻停下所有大型AI研究！”马斯克领衔，1000多名硅谷企业家科学家联名呼吁](https://mp.weixin.qq.com/s?__biz=MjM5NzAwMzU0MA==&mid=2247577418&idx=1&sn=3812ee078965613f04f53e0ef53de1cd&scene=21#wechat_redirect)

所以，我们不得不畅想，马斯克这么呼吁的目的到底是__作为一名清白的资本家呼吁人类拯救自己的未来__，还是想要__让OpenAI被迫暂停好让自己的GPT后来居上__？

从阴谋论的角度来说，答案不言而喻。

不过，好在我们这里不是讨论阴谋论的地方，所以暂且不考虑更深入的问题。

有趣的问题其实是：AI，尤其是现在的AI，到底会对人类的生活与社会带来什么样的冲击？

所以下面就从一些显而易见的地方着手，简单聊聊这个话题。

---

在我看来，AI，尤其是现阶段的AI可能引起的最大问题，在于**AI将改变我们获取信息、获取知识的途径**，而在此过程中无论是AI还是我们自己都可能会对我们的生活与社会带来负面影响。

在AI之前，我们获取信息的主要方式还是我们自己去接触信息源并接受信息。比如，无论是在朋友圈里看到还是算法推荐在我们的TimeLine上，我们至少都要自己去打开链接、浏览文章或视频，然后才算是完成了信息的接收过程。

但随着AI的出现，这个模式发生了改变，我们会更多倾向于选择先与AI接触，让AI为我们概述文章，然后再考虑是否去查看相应的文章，比如下面这张图所展示的：

![New Bing](/image/ai1.png)
![WriteSonic](/image/ai2.png)
![ChatGPT](/image/ai3.png)

上述三个AI分别是NewBing（GPT-4）、WriteSonic（GPT-3.5）和ChatGPT（GPT-3.5，加载了WebChatGPT插件来提供外部信息源）。

你看，在这个模式中，人们可以将自己的问题以接近日常用语的方式直接提问给AI，然后AI进行信息的搜索、概括与整合，然后用接近我们习惯用语的（语法上几乎没有错误的）方式反馈给我们。

和人直接进行搜索的区别在哪里？区别在于我们不再直面信息。

当我们在Google或者Bing中进行搜索的时候，其实相当于我们在一家大型信息超市中，我们看到的都是信息的标题与部分内容的摘抄，但并不直接被AI概括过，所以可以说我们直面的是__原始信息__，然后是由我们自己对信息进行筛选。

可现在我们不再直面原始信息，我们面对的是经过AI处理过的二次信息。

这样就带来两个方面的问题：

1.	信息量其实被严重压缩了：我们原本需要直面Google或Bing提供的大量信息，而现在我们只面对AI提供的少量信息了；
1.	我们可能会直接采用AI概述的信息而非原始信息，这样信息的质量与可靠性就存疑了。

前者其实不是新鲜玩意，在以今日头条为代表的推荐算法大行其道的前几年我们就已经感受到这玩意带来的冲击了，那便是__信息茧房__。

当我们需要面对海量未经筛选的信息（这里不说Google等搜索引擎对信息的排序可能带来的筛选问题）时，我们其实需要面对各种不同角度、不同深度、不同广度的思想，我们自己也会受到全方位的思想冲击，从而我们需要自己去思考、筛选、重塑思想。

但当信息的筛选工作由算法（无论是推荐算法还是AI）代劳后，我们便不会有机会去直面那些不同角度、不同深度、不同广度的思想，我们将被算法提供给我们的、料定我们会喜欢的信息以及AI筛选过的更符合AI自己思路的信息所包围。

而，一旦大脑所处的信息环境发生改变，那么它当然也会随之而变。其结果会有两个：

1.	因为使用大脑进行信息处理的机会变少了，所以我们可能会变得迟钝；
1.	因为一直接触同质信息，所以我们的思想可能会变得极端。

上述两个危害并不是我瞎掰的，在《劫持》这本书中临床心理学家玛丽便通过脑电图来直观地展示了网络对大脑的影响——当然，她所处的时代推荐算法尚未大行其道，所以那个年代便已经显现出来的影响在现在这个时代只可能更加严重。

事实上，过度使用网络等电子设备对大脑的影响不仅仅是变得迟钝与极端这两点，还可能会引起诸如多动症、注意力难以集中、类似癫痫一般的大脑过度活跃，等等等等。而在算法时代以及近在眼前的AI时代，人类演化缓慢的大脑是否能经受得住如海啸一般席卷而来的异质信息巨浪呢？没人知道。

另一方面，当人类不再直面原始信息取而代之的是AI提供的概述信息之后，还可能会带来其他方面的问题。

最直接的就是人类自己概述信息的机会变少了，从而进一步丧失了锻炼大脑的机会。

同时，概述信息本身会丢失原始信息中的很多细节。我们当然会说我们只需要一篇文章的最核心思想内容就可以了，但很多时候细节中蕴含的信息量是非常巨大的，和文章的核心思想是不相上下的。

更重要的是，如果我们只看AI为我们概述后的信息而不看原始信息的话，那么我们将无法判断我们看到的信息到底是不是正确的。

举例来说，最近正好看完了2021年诺贝尔文学奖得主古尔纳的小说《多蒂》，于是我们让AI来为我们提供一下这部小说的概述：

![由New Bing概述](ai4.png)

在这个概述中就反映了上面提到的两个问题：

首先，__这个概述信息将几乎所有细节都丢失了__，而《多蒂》这部小说真正打动人心的就在于那些细节，比如社区工作人员对多蒂的关照、默里医生对多蒂的脱帽礼、夜校老师提供的友谊与帮助，等等等等。这些真正闪耀光彩的点在这篇概述中能看到么？完全看不到。概述中提供的只是故事主线的精华，而没有其他。

而第二个问题，也是更重要的问题，那就是：__这篇概述根本不是《多蒂》的__。古尔纳的很多小说都有着相同的主题，比如在《多蒂》之前的《朝圣者之路》中，我们也能看到英国人的殖民主义为非洲人民留下的遗毒，也能看到非洲人移民到英格兰后所遭遇的歧视，但NewBing提供的这篇寻根概述和《多蒂》完全无关。可从这篇内容本身来看，我们能判断出AI在挂羊头卖狗肉么？并不能。

这还仅仅是文字，而今的AI已经可以合成图像（Midjourney与Stable Diffusion是最著名的两个），而在很多实验性项目中AI已经可以根据恰当的prompt合成视频，那么我们以后所看到的东西的真实性要如何去甄别呢？

>[info]	最近就有新闻，说有人用AI工具自动生成了地铁里一位路人女士的裸体照。再加上此前就已经出现的换脸AI，能把任何人的头换成另一个人的脸，这些技术日趋成熟后，有一天我们恐怕真的很难分辨“亲眼看到”的事件到底是不是真的。

当我们所接触到的一切文字、图像、视频、音频信息，甚至未来可期的VR、AR、MR环境，都是AI提供的，那么我们如何能知道自己所接受到的信息不是AI篡改过的呢？

这其实就是另一个版本的__缸中之脑__问题：我们虽然不是只有一个大脑悬浮在营养液之缸中，但我们实际上整悬浮在由AI调配出来的信息之缸中。

这是AI目前最大的问题。

除此之外，另一个问题就是__加强版的信息茧房__：由于AI需要消耗大量的算力资源，所以如果未来所有人的信息渠道都交由AI来代劳的话，那自然就存在一个问题，即所有人所选用的AI可能就那么少数几个，而AI虽然可能会根据每个人的不同喜好来给出不同的筛选结果，但由于AI就那么几个，从而推荐算法带来的信息茧房问题便可能在AI上重现，而且是以一种更加隐蔽同时也更加深层次的形式呈现的。这就好比如果世界上总共就只有三个语文老师的话，那么大家写的作文的多样性显然会比现在差很多。

而要从更深层次的角度来说的话，上面所提到的问题又有两个根本性的来源：

1.	人类的懒惰；
1.	技术的不成熟。

就我个人经验来看，人类的懒惰是最关键的问题，其严重程度远超技术的不成熟。

比如，如果人类都不懒惰的话，那么完全可以自己去搜寻信息而不用让AI代劳，而这样的话即便AI相关的技术还不成熟所以会带来各种问题，也不会成为问题，因为我们正在自己直面信息而没有假借AI之手，那自然不用担心AI给我们过度概括与错误的信息了。

但人类的懒惰却又是无法避免的问题——事实上，人类之所以能实现科技进步，就源于人们都在追求偷懒。

是偷懒这一第一动力，让我们发明了汽车来取代双脚与马匹；发明了电力来取代繁重的体力劳动；发明了计算机来取代机械式重复的脑力劳动。

因此，让人类放弃偷懒是反人性的、反自然的。

所以在很多有想法的设计师手中，他们有时会故意设计一些使用起来有一定麻烦的产品，其根本目的不是为了刁难用户，而是为了让用户不要过于依赖科技。这点在日本顶尖设计师、MUJI的品牌设计师原研哉的《设计的设计》中就有所阐述。

这就表示，一旦AI工具出现，那人们自然就会选择使用AI来完成工作而不再动用自己珍贵的大脑。

这个问题是约束AGI的研发能解决的么？

不可能。

真的要从根源上解决这问题的话，**我们不应该限制AGI的研发，而应该限制AGI的使用。**

可限制使用AGI是非常困难的——当然，比全面禁止使用AGI要靠谱，禁止AGI的使用无论从经济发展的角度还是从人类自身发展的角度来说，都是不可能的，就好比我们不能因为计算器的发明会妨碍人们自己算术而禁止人们使用计算器一样。

因此，一些国家已经宣布禁止在学校里使用ChatGPT等AI工具，这其实也是为了让学生们可以尽量自己动脑与动手来完成学习——如果学习的过程都由AI代劳的话，那到底是AI完成了大学学业还是人完成了大学学业呢？

但限制AGI的使用存在一个如何划定限制范围的问题。比如中小学在学习范围里不允许学生使用AI工具，这个目前看来或许是合理的。但大学呢？很多事情的确没必要大学生自己去做，那么此时也禁止使用AI就未免有点不合理了。

在工作场合就更难界定了——站在公司的立场，AGI提升效率，从而是一个好工具；但从员工的角度来说，AGI会不会取代自己不好说，工作都让AGI来完成了那自己其实也就丧失了进一步学习工作技巧与积累经验的机会，虽然这些技巧与经验在AGI大行其道的时代未必还有用，但如果这些经验都不积累的话，AGI真的大行其道的时候自己又能积累什么经验呢？

因此，限制AGI的使用是一个说起来绝对正确，但实际上却很难具备良好可操作性的建议。

更何况，这和马斯克等人宣称的暂缓AGI的研发完全是两码事——很多问题是随着系统的研发与使用才能暴露出来的，你暂缓研发其实不过是暂缓相关问题的暴露而已，并不能解决问题。

人类的懒惰是无可避免的，也是不应避免的，所以我们应该将目光聚焦在另一个问题上，那便是技术的不成熟。

不成熟的技术要想成熟，那么加大研发力度是一个很自然的思路。

但，就如马斯克等人所提到的，这里如何把握方向是一个非常重要的问题。

我们已经可以在一些黑市里看到不受限版的GPT-3了，这些系统未来如何得到进一步加强，成为不受限版的GPT-5或者更高版本的话，那情况显然就会变得如马斯克等人所想的那么不可控。

但这里有一个问题：__这个问题是暂缓研究AGI半年能解决的么？__

答案显然是否定的。我们见过警察开六个月会后，所有小偷与强盗统统自动缴械投降的么？

因此，事实上，除非马斯克等人可以将全球所有AI工具统统掌控在手，否则他们开半年的脑暴会从哲学、道德、伦理、技术层面来讨论如何为AI的发展指明方向，能解决AI未来可能被人为或自发地用在黑暗面么？这个问题来回答的必要都没有。

这也就是这份公开信伪善的一面——它所用的表面借口相当冠冕堂皇，根本找不出丝毫错漏之处，但实际上却是完全不具备可实现性的。

想要用AI搞事情的人，不会因为OpenAI停供半年而停止发展的脚步；而不想用AI搞事情的人，也不会因为马斯克等千余人没开脑暴会而造出一架天网。

我们当然需要就一些问题进行讨论，并形成新的行为规范。比如AIGC的内容的版权归属、AI引起的事故中的责任认定，等等。但这些问题本质上是人的问题而不是AGI的问题，是人所构成的社会如何将既有概念延拓到一个全新领域的问题，和AGI的研发没有任何关系。

还有一些问题则可能会和AGI研发有关，比如是否应该给AGI引入道德模块、数据集中的数据偏差可能引起的AGI认知偏见应该如何纠正，等等。但这些问题恰恰是现在技术中的缺漏，是需要尽快研究新的技术或者完善现有技术来解决的，而不是停下技术的脚步来进行哲学思辨能解决的。

将AGI应用到军事或犯罪事业上的风险？这个前面我们已经说过了，这不是一群马斯克开会就能有资格解决的问题。

那么AGI可能会引起人们的失业、冲击人类社会乃至人际关系呢？这个问题的唯一解决方案，前面提到过，就是全面禁止使用AGI，否则就是无解的，这和你现在停不停没有丝毫关系，不过就是延后问题的爆发而已。

由此可见，停下脚步来反思，这个说法的确很好，但对于解决目前AI所拥有的问题而言，其实根本没用。

这个口号之所以吸引人，是因为AI的发展速度太快，人类自己需要停下来喘口气、缓一缓，虽然缓完以后依然无法避免被AI的巨浪拍死的命运。

从这个角度来看，我认为AI的研发脚步不用停歇——事实上，要解决现在AI所带来的问题，就必须研究新的技术、完善现有技术，与此同时人类自己要做好迎接新世界的准备，包括将早就形成了的固有观念延拓到新世界的心理与智识准备。

当然，这并不是说我们完全不需要反思。

有时候发展得太快、太顺的时候，我们的确会忽略很多东西，此时停下来反思一下是应该的。

但集体停摆个半年来集体反思，这就有点扯淡了。
标题：从人列计算机看中文屋问题
作者：LostAbaddon
关键词：哲学 人工智能
更新：2023/03/11 18:07:39

在《三体》中，一个非常有趣的设定就是人列计算机。它可以认为是用士兵来构成逻辑门、队列就是电路，这么一种计算机。

>	想了解更多关于人列计算机的结构，可以看[这里](/article/science/physics/virtual/humancomputer.mu)。

于是，让我们来考虑这么一个问题：

**一群对一类特定偏微分方程问题一窍不通、只懂舞刀弄剑的士兵，排在特定的队列阵型，形成一台人列计算机。所有士兵都有一份指令集手册，告诉他们看到特定旗语后应该返回什么样的旗语回复。面对这台人列计算机的人不断向它递进关于那类特定偏微分方程的问题。构成人列计算机的士兵便按照指令集手册的说明，不断进行旗语传递，得出方程的解析解的符号表达，并将符号表达最终传出人列计算机，反馈给输入问题的人。那么，既然士兵没有一个是懂数学的，自然也就没有一个士兵是懂那类特定的偏微分方程的，所以人列计算机理应不懂这类数学问题——可它却解出了这些问题，这是为什么？**

这和美国哲学家约翰·罗杰斯·瑟尔于1980年发表于《行为与脑科学》杂志的论文《心灵、大脑和程序》一文中所提出的中文屋问题是非常像的：中文屋就是人列计算机，屋中人就是士兵，手册就是每个士兵手上的指令集手册，中文问题就是那类特殊的偏微分方程问题。

瑟尔认为，屋中人是不懂中文的，所以虽然手册让他对中文问题对答如流，但这个人不懂中文就是不懂中文，因此整个藏人屋也是不懂中文的——无论它回答中文问题时多么应答如流。因此，同样的，对于所有AI所要面对的需要意识来解决的问题，由于构成AI的硬件基础零件是没有意识的，所以作为它们所组成的整体的AI自然也是没有意识的，无论它对这类问题的回复是多么流利。

这两个问题是不是看起来很像？

事实上，利用人列计算机，我们可以把中文屋问题变得更加有趣：

现在整个藏人屋（就是里面坐着手拿手册的人类的那间屋子）是两台人列计算机（简称为人算机）而不再是电子计算机，一台是懂中文的士兵构成的人算机，一台是不懂中文的士兵构成的人算机，这两台人算机的队形与输入的程序都一模一样，对相同的中文问题所给出的答案自然也是一模一样的，这样从整体来看，这两部人算机是没有区别的。

然后让它来回答中文问题，并都能回答好，那么问题来了——这两台从整体来看一模一样的人算机，到底是都不懂中文，还是都懂中文？

显然，如果按照原本中文屋问题的思路，那由不懂中文的士兵构成的人算机应该是不懂中文的。可两台人算机一模一样，如果前者是不懂中文的，那么后者也是不懂中文的。

这便得出了一个有趣的结论：

{|}*由懂中文的人构成的、可以完美回复所有中文问题的人列计算机，是不懂中文的。*

这个结论未免过于荒谬。

这么看来，__由不懂中文的人构成的、可以完美回复所有中文问题的人列计算机，是可以懂中文的__。

事实上，我们可以构造这么两台人算机，一台是有士兵构建的，他们除了懂日常生活的尝试与军令之外，别的什么都不懂；另一台是由博士教授组成的，他们每一个都是人类已知的任何一个领域的绝对精英，世界上不存在比他们更懂某一个领域的知识的人。

我们就叫它们“士兵人算机”和“精英人算机”吧。

和之前一样，这两台人算机从功能角度来看，一模一样。

所以，问题就来了：他们对某一领域的问题给出的答案非常完美，那么士兵人算机和精英人算机到底是否懂得该领域的知识？

答案，就和上面所论的一样：__士兵人算机其实也懂任何领域的知识，虽然那些士兵本身并不懂__。

从这点来看，图灵机由什么构成并不重要，重要的是所要的功能是否实现了——这个论调倒是颇为功能主义。

事实上，我们可以将中文屋问题进一步拆解，它其实可以分为三个问题：

1.	子部分如果不具有某个属性的话，它们构成的整体是否一定不具有该属性？
2.	是否任意一个领域的问题，都可以有限长的指令集手册来给与完备的回答？
3.	如果不能真正理解某一领域的话，是否可能完备地回答该领域的问题？
4.	如果一个对象在行为上完全符合某个属性的要求，那么它是否可能其实不具备该属性？

其实，从现代复杂学的角度来看，__问题一的答案显然是否定的__，而这也是我们上面反复论证的。

第二个问题是一个非常有争议的问题。即便我们相信[邱奇-图灵论题](https://en.wikipedia.org/wiki/Church%E2%80%93Turing\_thesis)是正确的，那也不过说明一切可以用有限指令在有限步内完成的计算本质上都是图灵机可以解决的问题。但对于问题二中所涉及的“任意一个领域的问题”，显然是可以超越这个范畴的，比如[忙海狸问题](https://en.wikipedia.org/wiki/Busy\_beaver)。因此，如果泛泛而论的话，__问题二的答案应该是否定的__。可这依然无法解释这么一个问题：和意识相关的所有问题，是否都属于图灵不可解问题？对于此人们其实没有答案，只有信仰。

>[info]	有趣的是，我们总会用神经细胞并不具有意识，但在恰当的电化学刺激下构成的神经网络也就是人脑是拥有意识的，来反驳中文屋问题。但对这一反驳持反对观点的人则认为意识本就不是神经细胞通过电化学刺激所构成的神经网络所决定的，通过将精神与肉体剥离开这种二元身心论的观点来反驳也是非常有“创意”，但棘手的事人们其实也从来没证明过二元身心论就一定是错误的——神经科学还没发展到这个地步，而哲学家嘛，大家都懂的，吵归吵，结论是不会有的。

至于__问题三__，也是一个没有答案的问题，不过__人们普遍相信它的答案是否定__的这点的。

而问题四，说到底就是功能主义到底是否靠谱的问题，这目前更多是一个哲学问题，而哲学问题的回答是没有对错的。

所以，中文屋问题乃至人工智能是否有意识这类本不构成问题的问题的真正核心其实就是第二问：意识这么一个目前还没有定义的东西，是否可能通过有限长的操作手册来描述清楚？

这个问题恐怕短期内是误解喽。

>	当然在我上次写的[《从哥德尔与图灵开始的闲扯》](/article/science/math/fromgodeltogod.mu)里想了一个有趣的情况，嘿嘿。













